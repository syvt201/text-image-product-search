{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7896d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "mongo_uri = os.getenv(\"MONGO_URI\")\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[\"product_db\"]\n",
    "collection = db[\"products\"]\n",
    "\n",
    "# Test insert\n",
    "collection.insert_one({\"name\": \"red shoe\", \"path\": \"data/red_shoe.jpg\"})\n",
    "print(list(collection.find()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe2576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor\n",
    "\n",
    "# Load the processor\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Inspect the processor object\n",
    "print(dir(processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9cfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "model = load_model(os.path.join(\"../\", os.getenv(\"GROUDNINGDINO_CONFIG_PATH\")), \"../weights/groundingdino_swint_ogc.pth\")\n",
    "IMAGE_PATH = \"cat_dog.jpeg\"\n",
    "TEXT_PROMPT = \"chair . person . dog .\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)\n",
    "\n",
    "print(os.getenv(\"GROUDNINGDINO_CONFIG_PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be660d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# Chọn model\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "if model_type == \"vit_h\":\n",
    "    sam_checkpoint = \"../weights/sam_vit_h_4b8939.pth\" # 2.4gb\n",
    "elif model_type == \"vit_l\":\n",
    "    sam_checkpoint = \"../weights/sam_vit_l_0b3195.pth\" # 1.2gb\n",
    "elif model_type == \"vit_b\":\n",
    "    sam_checkpoint = \"../weights/sam_vit_b_01ec64.pth\" # 358mb\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Load ảnh test\n",
    "image = cv2.imread(\"cat_dog.jpeg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "predictor.set_image(image)\n",
    "\n",
    "\n",
    "# Box format: [x1, y1, x2, y2]\n",
    "box = np.array([[50, 50, 300, 300]])   # shape (1,4)\n",
    "masks, scores, logits = predictor.predict(\n",
    "    box=box,\n",
    "    multimask_output=True\n",
    ")\n",
    "\n",
    "print(\"Masks shape:\", masks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "model_name=\"openai/clip-vit-base-patch32\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(model_name, dtype=torch.bfloat16, attn_implementation=\"sdpa\", use_safetensors=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(model_name,  use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29640910",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=\"a photo of cat\", images=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", attn_implementation=\"sdpa\", use_safetensors=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812210c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids=inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(CLIPProcessor.__call__)\n",
    "# help(CLIPModel.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64880bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.signature(CLIPProcessor.__call__))\n",
    "print(inspect.signature(CLIPModel.__call__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965a7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the 'B' directory to sys.path\n",
    "sys.path.append(os.path.abspath(\"../src/models\"))\n",
    "\n",
    "from clip_encoder import CLIPEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6538055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129c71a0439a424daccfc0df75c060e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ce = CLIPEncoder(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaef7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"cat_dog.jpeg\").convert(\"RGB\")\n",
    "text = \"a photo of a cat\"\n",
    "\n",
    "text_embed, img_embed = ce.encode(text=None, image=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "146fe69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, (1, 512))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embed, img_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55c656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f357ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_safetensors=True, use_fast=True)\n",
    "model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\", attn_implementation=\"sdpa\", use_safetensors=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc839c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = processor.image_processor\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c4e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeded.pixel_values.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acce2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_image_features(**img_embeded).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenize = tokenizer(text, return_tensors='pt', padding=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_text_features(**text_tokenize).detach().cpu().numpy().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\", dtype=torch.bfloat16, attn_implementation=\"sdpa\", use_safetensors=True)\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_safetensors=True)\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]\n",
    "\n",
    "inputs = processor(text=labels, images=None, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "most_likely_idx = probs.argmax(dim=1).item()\n",
    "most_likely_label = labels[most_likely_idx]\n",
    "print(f\"Most likely label: {most_likely_label} with probability: {probs[0][most_likely_idx].item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd57952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", use_safetensors=True)\n",
    "print(model.config.projection_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

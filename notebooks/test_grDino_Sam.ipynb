{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0f8869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc7bfd81d2c479d8f5492a23c70cc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, GroundingDinoProcessor, GroundingDinoImageProcessor, GroundingDinoForObjectDetection, infer_device\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "device = infer_device()\n",
    "\n",
    "processor = GroundingDinoProcessor.from_pretrained(model_id)\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45471fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method __call__ in module transformers.models.grounding_dino.processing_grounding_dino:\n",
      "\n",
      "__call__(images: Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']] = None, text: Union[str, list[str], list[list[str]]] = None, audio=None, videos=None, **kwargs: typing_extensions.Unpack[transformers.models.grounding_dino.processing_grounding_dino.GroundingDinoProcessorKwargs]) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.grounding_dino.processing_grounding_dino.GroundingDinoProcessor instance\n",
      "    This method uses [`GroundingDinoImageProcessor.__call__`] method to prepare image(s) for the model, and\n",
      "    [`BertTokenizerFast.__call__`] to prepare text for the model.\n",
      "    \n",
      "    Args:\n",
      "        images (`ImageInput`, `list[ImageInput]`, *optional*):\n",
      "            The image or batch of images to be processed. The image might be either PIL image, numpy array or a torch tensor.\n",
      "        text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`, *optional*):\n",
      "            Candidate labels to be detected on the image. The text might be one of the following:\n",
      "            - A list of candidate labels (strings) to be detected on the image (e.g. [\"a cat\", \"a dog\"]).\n",
      "            - A batch of candidate labels to be detected on the batch of images (e.g. [[\"a cat\", \"a dog\"], [\"a car\", \"a person\"]]).\n",
      "            - A merged candidate labels string to be detected on the image, separated by \".\" (e.g. \"a cat. a dog.\").\n",
      "            - A batch of merged candidate labels text to be detected on the batch of images (e.g. [\"a cat. a dog.\", \"a car. a person.\"]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(processor.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/mnt/e/text-image-product-search/images/000000039769.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8afc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_labels = [[\"plant pot\", \"book\", \"table\", \"chair\", \"carpet\", \"rug\"]] # text khong duoc co '.'\n",
    "text_labels = [[\"a cat\", \"a remote control\"]]\n",
    "# text_labels = [\"a cat.\", \"a remote control.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc444db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, text=text_labels, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fabda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "127e86cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs=outputs,\n",
    "    input_ids=inputs.input_ids,\n",
    "    threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33db88ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "result = results[0]\n",
    "for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]):\n",
    "    # box = [round(x, 2) for x in box.tolist()]\n",
    "    # print(f\"Detected {labels} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "    print(box.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f641562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([344.6931,  23.1090, 637.1847, 374.2747], device='cuda:0'),\n",
       " tensor(0.4785, device='cuda:0'),\n",
       " 'a cat')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37a65dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[344.69305419921875, 23.109039306640625, 637.1846923828125, 374.274658203125],\n",
       " [12.26496696472168, 51.915008544921875, 316.8591003417969, 472.4386901855469],\n",
       " [38.58332061767578, 70.0059585571289, 176.77804565429688, 118.17623901367188]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][\"boxes\"].detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6f9e8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scores': tensor([0.4785, 0.4381, 0.4759], device='cuda:0'),\n",
       " 'boxes': tensor([[344.6931,  23.1090, 637.1847, 374.2747],\n",
       "         [ 12.2650,  51.9150, 316.8591, 472.4387],\n",
       "         [ 38.5833,  70.0060, 176.7780, 118.1762]], device='cuda:0'),\n",
       " 'text_labels': ['a cat', 'a cat', 'a remote control'],\n",
       " 'labels': ['a cat', 'a cat', 'a remote control']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9631f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method __call__ in module transformers.models.grounding_dino.processing_grounding_dino:\n",
      "\n",
      "__call__(images: Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']] = None, text: Union[str, list[str], list[list[str]]] = None, audio=None, videos=None, **kwargs: typing_extensions.Unpack[transformers.models.grounding_dino.processing_grounding_dino.GroundingDinoProcessorKwargs]) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.grounding_dino.processing_grounding_dino.GroundingDinoProcessor instance\n",
      "    This method uses [`GroundingDinoImageProcessor.__call__`] method to prepare image(s) for the model, and\n",
      "    [`BertTokenizerFast.__call__`] to prepare text for the model.\n",
      "    \n",
      "    Args:\n",
      "        images (`ImageInput`, `list[ImageInput]`, *optional*):\n",
      "            The image or batch of images to be processed. The image might be either PIL image, numpy array or a torch tensor.\n",
      "        text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`, *optional*):\n",
      "            Candidate labels to be detected on the image. The text might be one of the following:\n",
      "            - A list of candidate labels (strings) to be detected on the image (e.g. [\"a cat\", \"a dog\"]).\n",
      "            - A batch of candidate labels to be detected on the batch of images (e.g. [[\"a cat\", \"a dog\"], [\"a car\", \"a person\"]]).\n",
      "            - A merged candidate labels string to be detected on the image, separated by \".\" (e.g. \"a cat. a dog.\").\n",
      "            - A batch of merged candidate labels text to be detected on the batch of images (e.g. [\"a cat. a dog.\", \"a car. a person.\"]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(processor.__call__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e583b0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77f00737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa3483fc09748deb0c2f3526597e582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2447af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method __call__ in module transformers.models.sam.processing_sam:\n",
      "\n",
      "__call__(images: Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None, text: Union[str, list[str], list[list[str]], NoneType] = None, audio: Union[ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), list['np.ndarray'], list['torch.Tensor'], NoneType] = None, video: Union[list['PIL.Image.Image'], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), list['np.ndarray'], list['torch.Tensor'], list[list['PIL.Image.Image']], list[list['np.ndarrray']], list[list['torch.Tensor']], transformers.video_utils.URL, list[transformers.video_utils.URL], list[list[transformers.video_utils.URL]], transformers.video_utils.Path, list[transformers.video_utils.Path], list[list[transformers.video_utils.Path]], NoneType] = None, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.sam.processing_sam.SamProcessor instance\n",
      "    This method uses [`SamImageProcessor.__call__`] method to prepare image(s) for the model. It also prepares 2D\n",
      "    points and bounding boxes for the model if they are provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(processor.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bb69db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from datetime import datetime, timezone\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "def get_image_metadata(file_path):\n",
    "    stat_info = os.stat(file_path)\n",
    "    size = stat_info.st_size   # (bytes)\n",
    "    uploaded_at = datetime.fromtimestamp(stat_info.st_mtime, tz=timezone.utc)\n",
    "    \n",
    "    with Image.open(file_path) as img:\n",
    "        width, height = img.size\n",
    "        format = img.format.lower()  # jpg, png, webp,...\n",
    "\n",
    "    return {\n",
    "        \"file_name\": os.path.basename(file_path),\n",
    "        \"url\": os.path.abspath(file_path),\n",
    "        \"uploaded_at\": uploaded_at.isoformat(),\n",
    "        \"size\": size,\n",
    "        \"format\": format,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    }\n",
    "    \n",
    "    \n",
    "def mask_to_polygon(mask: np.ndarray) -> list[list[int]]:\n",
    "    # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Extract the vertices of the contour\n",
    "    polygon = largest_contour.reshape(-1, 2).tolist()\n",
    "\n",
    "    return polygon\n",
    "\n",
    "def polygon_to_mask(polygon: list[tuple[int, int]], image_shape: tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a polygon to a segmentation mask.\n",
    "\n",
    "    Args:\n",
    "    - polygon (list): List of (x, y) coordinates representing the vertices of the polygon.\n",
    "    - image_shape (tuple): Shape of the image (height, width) for the mask.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Segmentation mask with the polygon filled.\n",
    "    \"\"\"\n",
    "    # Create an empty mask\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "\n",
    "    # Convert polygon to an array of points\n",
    "    pts = np.array(polygon, dtype=np.int32)\n",
    "\n",
    "    # Fill the polygon with white color (255)\n",
    "    cv2.fillPoly(mask, [pts], color=(255,))\n",
    "\n",
    "    return mask\n",
    "\n",
    "def get_boxes(results: list[dict]) -> list[list[list[float]]]:\n",
    "    # `results` are the detection results from GroundingDino. This is a list of dictionaries, with each dictionary containing the following keys:\n",
    "    #   \"scores: The confidence scores for each predicted box on the image.\n",
    "    #   \"labels: Indexes of the classes predicted by the model on the image.\n",
    "    #   \"boxes: Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n",
    "    \n",
    "    list_boxes = []\n",
    "    for result in results:\n",
    "        boxes = result[\"boxes\"].detach().cpu().tolist()\n",
    "        list_boxes.append(boxes)\n",
    "        \n",
    "    return list_boxes\n",
    "\n",
    "def refine_masks(masks : torch.Tensor, polygon_refinement: bool = False):\n",
    "    masks = masks.detach().cpu().float()\n",
    "    if masks.ndim == 4:   # [B, C, H, W]\n",
    "        masks = masks.permute(0, 2, 3, 1).mean(dim=-1)  # [B, H, W]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected mask shape: {masks.shape}\")\n",
    "    masks = masks.mean(axis=-1)\n",
    "    masks = (masks > 0).int()\n",
    "    masks = masks.numpy().astype(np.uint8)\n",
    "    masks = list(masks)\n",
    "\n",
    "    if polygon_refinement:\n",
    "        for idx, mask in enumerate(masks):\n",
    "            shape = mask.shape\n",
    "            polygon = mask_to_polygon(mask)\n",
    "            mask = polygon_to_mask(polygon, shape)\n",
    "            masks[idx] = mask\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6fd04163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GroundingDinoProcessor, GroundingDinoForObjectDetection, infer_device\n",
    "from PIL import Image\n",
    "import string\n",
    "import torch\n",
    "\n",
    "class GroundingDinoDetector:\n",
    "    def __init__(self, model_id: str=\"IDEA-Research/grounding-dino-tiny\", device=None):\n",
    "        self.device = device or infer_device() \n",
    "        self.processor = GroundingDinoProcessor.from_pretrained(model_id)\n",
    "        self.model = GroundingDinoForObjectDetection.from_pretrained(model_id).to(self.device)\n",
    "    \n",
    "    def detect(self, text:str | list[str], image: str | list[str] | Image.Image | list[Image.Image], threshold: float = 0.4, text_threshold: float = 0.3):\n",
    "        \"\"\"\n",
    "        Detect objects in the image based on the text prompt.\n",
    "        Args:\n",
    "            text (str or list of str): Text prompt(s) for object detection.\n",
    "            image (str or list of str or Image or list of Image): Path(s) to image file(s) or PIL Image(s)\n",
    "            threshold (float): Threshold to keep object detection predictions based on confidence score.\n",
    "            text_threshold (float): Score threshold to keep text detection predictions.\n",
    "        Returns:\n",
    "            list[Dict]: A list of dictionaries, each dictionary containing the\n",
    "                scores: tensor of confidence scores for detected objects\n",
    "                boxes: tensor of bounding boxes in [x0, y0, x1, y1] format\n",
    "                labels: list of text labels for each detected object (will be replaced with integer ids in v4.51.0)\n",
    "                text_labels: list of text labels for detected objects\n",
    "        \"\"\"\n",
    "        \n",
    "        if text is None or image is None:\n",
    "            raise ValueError(\"Both `text` and `image` must be provided\")\n",
    "        \n",
    "        if not isinstance(image, list):\n",
    "            image = [image]\n",
    "            \n",
    "        pil_images = []\n",
    "        for img in image:\n",
    "            if isinstance(img, str):\n",
    "                with Image.open(img) as im:\n",
    "                    pil_images.append(im.convert(\"RGB\").copy())\n",
    "            elif isinstance(img, Image.Image):\n",
    "                pil_images.append(img if img.mode == \"RGB\" else img.convert(\"RGB\"))\n",
    "            else:\n",
    "                raise ValueError(\"`image` must be a file path, a PIL Image, or a list of either\")\n",
    "        \n",
    "        # remove punctuation from text\n",
    "        if isinstance(text, list):\n",
    "            text_labels = [label.translate(str.maketrans('', '', string.punctuation)) for label in text]\n",
    "        else:\n",
    "            text_labels = [text.translate(str.maketrans('', '', string.punctuation))]\n",
    "        \n",
    "        # Preproces\n",
    "        inputs = self.processor(images=pil_images, text=text_labels, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Post-process results\n",
    "        results = self.processor.post_process_grounded_object_detection(\n",
    "            outputs=outputs,\n",
    "            input_ids=inputs.input_ids,\n",
    "            threshold=threshold,\n",
    "            text_threshold=text_threshold,\n",
    "            target_sizes=[pil_images[0].size[::-1]]\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "from transformers import SamModel, SamProcessor, infer_device\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class SamSegmentator:\n",
    "    def __init__(self, model_id=\"facebook/sam-vit-base\", device=None):\n",
    "        self.device = device or infer_device()\n",
    "        self.processor = SamProcessor.from_pretrained(model_id)\n",
    "        self.model = SamModel.from_pretrained(model_id).to(self.device)\n",
    "    \n",
    "    def segment(self, \n",
    "                image: str | list[str] | Image.Image | list[Image.Image], \n",
    "                detection_results: list[dict],\n",
    "                polygon_refinement: bool = False\n",
    "    ):\n",
    "        \n",
    "        if image is None or detection_results is None:\n",
    "            raise ValueError(\"Both `image` and `detection_results` must be provided\")\n",
    "        \n",
    "        if not isinstance(image, list):\n",
    "            image = [image]\n",
    "        \n",
    "        pil_images = []\n",
    "        for img in image:\n",
    "            if isinstance(img, str):\n",
    "                with Image.open(img) as im:\n",
    "                    pil_images.append(im.convert(\"RGB\").copy())\n",
    "            elif isinstance(img, Image.Image):\n",
    "                pil_images.append(img if img.mode == \"RGB\" else img.convert(\"RGB\"))\n",
    "            else:\n",
    "                raise ValueError(\"`image` must be a file path, a PIL Image, or a list of either\")\n",
    "            \n",
    "        boxes = get_boxes(results=detection_results)\n",
    "        inputs = self.processor(images=pil_images, input_boxes=boxes, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        masks = self.processor.post_process_masks(\n",
    "            masks=outputs.pred_masks,\n",
    "            original_sizes=inputs.original_sizes,\n",
    "            reshaped_input_sizes=inputs.reshaped_input_sizes\n",
    "        )[0]\n",
    "        \n",
    "        masks = refine_masks(masks, polygon_refinement)\n",
    "        # for detection_result, mask in zip(detection_results, masks):\n",
    "        #     detection_result.mask = mask\n",
    "\n",
    "        # return detection_results \n",
    "        return detection_results, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b19982e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d0a2385dba4d1b84aa90a60870f571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad40faffbe54be29ed8011f1a513d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detector = GroundingDinoDetector()\n",
    "segmentator = SamSegmentator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af121fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"a cat.\", \"a remote control.\"]\n",
    "img_path = \"/mnt/e/text-image-product-search/images/000000039769.jpg\"\n",
    "# result = detector.detect(text=labels, image_path=img_path)\n",
    "detection_results = detector.detect(text=labels, image=img_path)\n",
    "detection_results_1, masks = segmentator.segment(image=image_path, detection_results=detection_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "167950ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syvt/miniconda3/envs/img_search/lib/python3.10/site-packages/transformers/models/grounding_dino/processing_grounding_dino.py:94: FutureWarning: The key `labels` is will return integer ids in `GroundingDinoProcessor.post_process_grounded_object_detection` output since v4.51.0. Use `text_labels` instead to retrieve string object names.\n",
      "  warnings.warn(self.message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'scores': tensor([0.4785, 0.4381, 0.4759], device='cuda:0'),\n",
       " 'boxes': tensor([[344.6931,  23.1090, 637.1847, 374.2747],\n",
       "         [ 12.2650,  51.9150, 316.8591, 472.4387],\n",
       "         [ 38.5833,  70.0060, 176.7780, 118.1762]], device='cuda:0'),\n",
       " 'text_labels': ['a cat', 'a cat', 'a remote control'],\n",
       " 'labels': ['a cat', 'a cat', 'a remote control']}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ea6a7b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25666977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0733bf772365462092a0ba9ab61fa20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5e31726ee14aa08bb3f4fd4794cf98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskGeneration, AutoProcessor\n",
    "segmenter_id = None\n",
    "segmenter_id = segmenter_id if segmenter_id is not None else \"facebook/sam-vit-base\"\n",
    "\n",
    "segmentator = AutoModelForMaskGeneration.from_pretrained(segmenter_id).to(device)\n",
    "processor = AutoProcessor.from_pretrained(segmenter_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a16de5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.sam.modeling_sam.SamModel,\n",
       " transformers.models.sam.processing_sam.SamProcessor)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(segmentator), type(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a1261068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method __call__ in module transformers.models.sam.processing_sam:\n",
      "\n",
      "__call__(images: Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None, text: Union[str, list[str], list[list[str]], NoneType] = None, audio: Union[ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), list['np.ndarray'], list['torch.Tensor'], NoneType] = None, video: Union[list['PIL.Image.Image'], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), list['np.ndarray'], list['torch.Tensor'], list[list['PIL.Image.Image']], list[list['np.ndarrray']], list[list['torch.Tensor']], transformers.video_utils.URL, list[transformers.video_utils.URL], list[list[transformers.video_utils.URL]], transformers.video_utils.Path, list[transformers.video_utils.Path], list[list[transformers.video_utils.Path]], NoneType] = None, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.sam.processing_sam.SamProcessor instance\n",
      "    This method uses [`SamImageProcessor.__call__`] method to prepare image(s) for the model. It also prepares 2D\n",
      "    points and bounding boxes for the model if they are provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(processor.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "44d95736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 11], [18, 12], [18, 180], [19, 181], [228, 181], [229, 180], [229, 12], [227, 12], [226, 11]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Đọc ảnh\n",
    "img = cv2.imread(\"/mnt/e/text-image-product-search/images/canny-edges.png\")\n",
    "if img is None:\n",
    "    raise ValueError(\"file not found or not an image\")\n",
    "\n",
    "# Chuyển sang grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Threshold để tạo ảnh nhị phân\n",
    "_, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Tìm contour\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# print(f\"Found {len(contours)} contours\")\n",
    "\n",
    "# # Vẽ contour lên ảnh copy\n",
    "# img_contour = img.copy()\n",
    "# cv2.drawContours(img_contour, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# # Hiển thị kết quả\n",
    "# cv2.imshow(\"Original\", img)\n",
    "# cv2.imshow(\"Threshold\", thresh)\n",
    "# cv2.imshow(\"Contours\", img_contour)\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Extract the vertices of the contour\n",
    "polygon = largest_contour.reshape(-1, 2).tolist()\n",
    "print(polygon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7fffd8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " (12, 1, 2),\n",
       " array([[[ 34, 277]],\n",
       " \n",
       "        [[ 34, 279]],\n",
       " \n",
       "        [[ 36, 281]],\n",
       " \n",
       "        [[ 37, 280]],\n",
       " \n",
       "        [[ 38, 281]]], dtype=int32))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contours), contours[0].shape, contours[0][:5]  # first 5 points of the first contour"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
